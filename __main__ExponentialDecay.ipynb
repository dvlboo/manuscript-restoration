{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Decay Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cek Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = np.load('data/train_samples.npz')\n",
    "ori_samples = np.load('data/ori_train.npz')\n",
    "\n",
    "test_samples = np.load('data/test_samples.npz')\n",
    "\n",
    "\n",
    "print(\"Keys in train_samples:\", list(train_samples.keys()))\n",
    "print(\"Keys in ori_samples:\", list(ori_samples.keys()))\n",
    "print(\"Keys in test_samples:\", list(test_samples.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_samples['train_images']\n",
    "y_train = train_samples['train_labels']\n",
    "x_ori = ori_samples['ori_train']\n",
    "x_test = test_samples['test_images']\n",
    "y_test = test_samples['test_labels']\n",
    "\n",
    "# cek shape dari data train dan test\n",
    "print(f\"x_train shape => {x_train.shape} || x_test shape => {x_test.shape} || x_ori shape => {x_ori.shape}\")\n",
    "\n",
    "# cek type dari data train dan test\n",
    "print(f\"x_train type => {x_train.dtype} || x_test type => {x_test.dtype} || x_ori type => {x_ori.dtype}\")\n",
    "\n",
    "#  cek jumlah label yang ada di data train dan test\n",
    "print(f\"Jumlah label train : {len(np.unique(y_train))} => {np.unique(y_train)}\")\n",
    "print(f\"Jumlah label test : {len(np.unique(y_test))} => {np.unique(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping label ke deskripsi\n",
    "label_descriptions = {\n",
    "\t'hole': \"Berlubang\",\n",
    "\t'bleed': \"Tinta Tembus\",\n",
    "\t'stain': \"Bercak\",\n",
    "\t'missing': \"Teks Hilang\"\n",
    "}\n",
    "\n",
    "# Modifikasi fungsi visualisasi\n",
    "def visualize_images(images, labels, indices, label_descriptions):\n",
    "\tnum_images = len(indices)\n",
    "\tfig, axes = plt.subplots(1, num_images, figsize=(15, 5))\n",
    "\n",
    "\tfor i, idx in enumerate(indices):\n",
    "\t\taxes[i].imshow(images[idx].squeeze(), cmap='gray')\n",
    "\t\taxes[i].set_title(f\"{label_descriptions[labels[idx]]}\")\n",
    "\t\taxes[i].axis('off')\n",
    "\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index_train\n",
    "train_indices = [5, 25, 45, 65]\n",
    "\n",
    "# visualisasi data train\n",
    "visualize_images(x_train, y_train, train_indices, label_descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisasi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi normalisasi data\n",
    "def normalize_data(img):\n",
    "\t# Mengilangkan channel warna jika perlu\n",
    "\tif img.shape[-1] == 3:\n",
    "\t\timg = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\t\n",
    "\t# dtype float32\n",
    "\timg = img.astype('float32')\n",
    "\t\n",
    "\t# Normalisasi data Tanh\n",
    "\timg = (img - 127.5) / 127.5\n",
    "\n",
    "\t# Ubah channel ke 1\n",
    "\timg = np.expand_dims(img, axis=-1)\n",
    "\t\n",
    "\treturn img\n",
    "\n",
    "# Normalisasi data train dan test\n",
    "x_train = np.array([normalize_data(img) for img in x_train])\n",
    "x_test = np.array([normalize_data(img) for img in x_test])\n",
    "x_ori = np.array([normalize_data(img) for img in x_ori])\n",
    "\n",
    "# cek shape dari data train dan test\n",
    "print(f\"x_train shape => {x_train.shape} || x_test shape => {x_test.shape} || x_ori shape => {x_ori.shape}\")\n",
    "\n",
    "# cek type dari data train dan test\n",
    "print(f\"x_train type => {x_train.dtype} || x_test type => {x_test.dtype} || x_ori type => {x_ori.dtype}\")\n",
    "\n",
    "# Cek normalisasi citra\n",
    "print(f\"Nilai pixel train : min => {x_train.min()} & max => {x_train.max()}\")\n",
    "print(f\"Nilai pixel test : min => {x_test.min()} & max => {x_test.max()}\")\n",
    "print(f\"Nilai pixel ori : min => {x_ori.min()} & max => {x_ori.max()}\")\n",
    "\n",
    "# Cek rata-rata pixel normalisasi min dan max\n",
    "print(f\"Rata-rata pixel train  min : {np.mean(x_train.min())}\")\n",
    "print(f\"Rata-rata pixel train  max : {np.mean(x_train.max())}\")\n",
    "\n",
    "print(f\"Rata-rata pixel ori min : {np.mean(x_ori.min())}\")\n",
    "print(f\"Rata-rata pixel ori max : {np.mean(x_ori.max())}\")\n",
    "\n",
    "print(f\"Rata-rata pixel test min : {np.mean(x_test.min())}\")\n",
    "print(f\"Rata-rata pixel test max : {np.mean(x_test.max())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index_train\n",
    "train_indices = [11, 31, 51, 71]\n",
    "\n",
    "# visualisasi data train\n",
    "visualize_images(x_train, y_train, train_indices, label_descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# import keras\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "# loss_function\n",
    "from utils.loss_function import generator_loss, discriminator_loss\n",
    "\n",
    "# import models\n",
    "from models import buildGen, buildDisc\n",
    "\n",
    "# import matrics\n",
    "from utils.metrics import psnr, ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "damaged_images = tf.data.Dataset.from_tensor_slices(x_train).batch(16)\n",
    "real_images = tf.data.Dataset.from_tensor_slices(x_ori).batch(16)\n",
    "\n",
    "# Melihat shape\n",
    "for x_img, y_img in zip(damaged_images.take(1), real_images.take(1)):\n",
    "\tprint(x_img.shape)\n",
    "\tprint(y_img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "\n",
    "# Cosine decay steps\n",
    "batch_size = 16\n",
    "batches = len(x_train) / batch_size\n",
    "decay_steps = batches * epochs\n",
    "\n",
    "# Learning rate decay\n",
    "fix_lr = 0.001\n",
    "exp_lr = ExponentialDecay(\n",
    "\tinitial_learning_rate=fix_lr, decay_steps=decay_steps, decay_steps=0.9\n",
    ")\n",
    "\n",
    "# Optimizer\n",
    "optimizer_g_cosine = Adam(learning_rate=exp_lr)\n",
    "optimizer_d = Adam(learning_rate=fix_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training generator\n",
    "def training_generator(generator, discriminator, real_images, damaged_images, optimizer):\n",
    "\twith tf.GradientTape() as tape:\n",
    "\t\tfake_images = generator(damaged_images, training=True)\n",
    "\t\tfake_output = discriminator([fake_images, damaged_images], training=False)\n",
    "\t\tgen_loss = generator_loss(real_images, fake_images, fake_output)\n",
    "\n",
    "\tgradients = tape.gradient(gen_loss, generator.trainable_variables)\n",
    "\toptimizer.apply_gradients(zip(gradients, generator.trainable_variables))\n",
    "\n",
    "\t# psnr_score = psnr(real_images, fake_images)\n",
    "\t# ssim_score = ssim(real_images, fake_images)\n",
    "\t\n",
    "\t# print(f\"Generator Loss: {gen_loss.numpy()}\")\n",
    "\t# print(f\"PSNR: {psnr_score.numpy()} || SSIM: {ssim_score.numpy()}\")\n",
    "\t\n",
    "\treturn gen_loss\n",
    "\n",
    "# Build Generator\n",
    "generator = buildGen()\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Discriminator\n",
    "def training_discriminator(discriminator, generator, real_images, damaged_images, optimizer):\n",
    "\twith tf.GradientTape() as tape:\n",
    "\t\tfake_images = generator(damaged_images, training=True)\n",
    "\t\treal_output = discriminator([real_images, damaged_images], training=True)\n",
    "\t\tfake_output = discriminator([fake_images, damaged_images], training=True)\n",
    "\t\tdisc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "\tgradients = tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\toptimizer.apply_gradients(zip(gradients, discriminator.trainable_variables))\n",
    "\n",
    "\t# print(f\"Discriminator Loss: {disc_loss.numpy()}\")\n",
    "\treturn disc_loss\n",
    "\n",
    "# Build Discriminator\n",
    "discriminator = buildDisc()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_images(damaged_images, real_images, generated_images, num_samples=2):\n",
    "\tplt.figure(figsize=(15, 5))\n",
    "\tfor i in range(num_samples):\n",
    "\t\t# Damaged\n",
    "\t\tplt.subplot(3, num_samples, i + 1)\n",
    "\t\tplt.imshow(np.squeeze(damaged_images[i]), cmap='gray')\n",
    "\t\tplt.title(\"Damaged\")\n",
    "\t\tplt.axis(\"off\")\n",
    "\n",
    "\t\t# Real\n",
    "\t\tplt.subplot(3, num_samples, i + 1 + num_samples)\n",
    "\t\tplt.imshow(np.squeeze(real_images[i]), cmap='gray')\n",
    "\t\tplt.title(\"Real\")\n",
    "\t\tplt.axis(\"off\")\n",
    "\n",
    "\t\t# Generated\n",
    "\t\tplt.subplot(3, num_samples, i + 1 + 2 * num_samples)\n",
    "\t\tplt.imshow(np.squeeze(generated_images[i]), cmap='gray')\n",
    "\t\tplt.title(\"Generated\")\n",
    "\t\tplt.axis(\"off\")\n",
    "\tplt.tight_layout()\n",
    "\tplt.show()\n",
    "\n",
    "\n",
    "def plot_loss(generator_losses, discriminator_losses):\n",
    "\tplt.figure(figsize=(10, 5))\n",
    "\tplt.plot(generator_losses, label='Generator Loss', color='blue')\n",
    "\tplt.plot(discriminator_losses, label='Discriminator Loss', color='red')\n",
    "\tplt.xlabel('Epochs')\n",
    "\tplt.ylabel('Loss')\n",
    "\tplt.title('Training Losses')\n",
    "\tplt.legend()\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(generator, discriminator, damaged_dataset, real_dataset, epochs):\n",
    "\t# Gabungkan dataset menjadi satu\n",
    "\ttrain_dataset = tf.data.Dataset.zip((damaged_dataset, real_dataset))\n",
    "\n",
    "\tfor epoch in range(epochs):\n",
    "\t\tstart_time = time.time()\n",
    "\t\tgen_loss_list = []\n",
    "\t\tdisc_loss_list = []\n",
    "\n",
    "\t\twith tqdm(total=batches , desc=f\"Epoch {epoch+1}/{epochs}\", unit=\"batch\") as pbar:\n",
    "\t\t\tfor damaged_batch, real_batch in train_dataset:\n",
    "\t\t\t\t# Latih discriminator\n",
    "\t\t\t\tdisc_loss = training_discriminator(discriminator, generator, real_batch, damaged_batch, optimizer_d)\n",
    "\t\t\t\t# Latih generator\n",
    "\t\t\t\tgen_loss = training_generator(generator, discriminator, real_batch, damaged_batch, optimizer_g_cosine)\n",
    "\n",
    "\t\t\t\tgen_loss_list.append(gen_loss.numpy())\n",
    "\t\t\t\tdisc_loss_list.append(disc_loss.numpy())\n",
    "\n",
    "\t\t\t\t# Update progress bar\n",
    "\t\t\t\tpbar.set_postfix(Gen_Loss=gen_loss.numpy(), Disc_Loss=disc_loss.numpy())\n",
    "\t\t\t\tpbar.update(1)\n",
    "\n",
    "\t\t\t\t# visualisasi hasil setiap 10 epoch\n",
    "\t\t\t\tif epoch % 10 == 0:\n",
    "\t\t\t\t\tgenerated_images = generator(damaged_batch, training=False)\n",
    "\t\t\t\t\tvisualize_images(damaged_batch, real_batch, generated_images, num_samples=2)\n",
    "\n",
    "\t\t# Hitung waktu epoch dan rata-rata loss\n",
    "\t\tepoch_duration = time.time() - start_time\n",
    "\t\tavg_gen_loss = sum(gen_loss_list) / len(gen_loss_list)\n",
    "\t\tavg_disc_loss = sum(disc_loss_list) / len(disc_loss_list)\n",
    "\n",
    "\t\tprint(f\"Epoch {epoch+1}/{epochs} completed in {epoch_duration:.2f}s\")\n",
    "\t\tprint(f\"Average Generator Loss: {avg_gen_loss:.4f}, Average Discriminator Loss: {avg_disc_loss:.4f}\\n\")\n",
    "\n",
    "train_model(generator, discriminator, damaged_images, real_images, epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
